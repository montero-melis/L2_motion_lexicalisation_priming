---
title: "Analysis replication and supplementary information"
author: '[Guillermo Montero-Melis](http://www.biling.su.se/montero_melis_guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
  word_document:
    toc: yes
---



# Set up working space

## Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 5)
```

```{r, include = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
library(dplyr)
packageVersion('dplyr')
library(tidyr)
packageVersion('tidyr')  # useful for gather() to convert from wide to long
library(mgcv)  # GAMs and GAMMs (Wood 2006)
packageVersion('mgcv')  # GAMs and GAMMs (Wood 2006)
library(itsadug)
packageVersion('itsadug')
library(lme4)
packageVersion('lme4')
library(ggplot2)
packageVersion('ggplot2')
library(boot)  # for inv.logit()
packageVersion('boot')  # for inv.logit()
library(knitr)  # for kable()
packageVersion('knitr')  # for kable()
library(lazyeval)  # lazy evaluation used in bysubj() function [summarise_]
packageVersion('lazyeval')
library(effects)
library(xtable)
```


## Data loading and processing

### Load data

```{r}
##  Load data
# The data is created in the script 'processing/compute_dependent_measures.R'
# There is the normal and the liberally coded version, see script for difference

# load
d <- read.csv('../data/data_DVs.csv', fileEncoding = 'UTF-8', stringsAsFactors = TRUE)
# simplify somewhat
d <- d %>%
  select(Subject:VideoName, P_V, M_V) %>%
  rename(Trial = VideoTrial)
# head(d)
# str(d)

# Rename "Control" condition to "Baseline"
levels(d$Condition)[levels(d$Condition) == "Control"] <- "Baseline"

# participant data
ppts <- read.csv("../data/participants.csv", fileEncoding = "UTF-8", stringsAsFactors = TRUE)
# head(ppts)
# No audio data recorded for Subject 14 (L2) due to experimental error; exclude
ppts <- ppts[ppts$Subject != 14, ]
# transform ClozeScore to z-score (as.vector prevents it from becoming a matrix)
ppts$zClozeScore <- as.vector(scale(ppts$ClozeScore))
# center ClozeScore but not scaling (as.vector prevents it from becoming a matrix)
ppts$cClozeScore <- as.vector(scale(ppts$ClozeScore, scale = FALSE))

# add speakers' clozescore to d:
d <- left_join(d, ppts %>% select(Subject, ClozeScore, zClozeScore, cClozeScore))

# Subject needs to be a factor
d$Subject <- factor(d$Subject)

# in Group, let the native speakers (NS) be the reference group
d$Group <- factor(d$Group, levels = c('NS', 'L2'))

head(d)
str(d)
```

```{r}
# Data for translation task
transl <- read.csv("../data/L2_translation-task_scored.csv")
getwd()
```



### Remove outliers?

Remove any L2 speakers based on cloze score because too low?

```{r}
# Outliers? ---------------------------------------------------------------

# L2 speakers Cloze scores
cloze <- ppts %>%
  filter(Group == "L2") %>%
  select(Subject, Condition, ClozeScore, zClozeScore)
# Subjects ordered by clozescore
head(cloze[order(cloze$zClozeScore), ])
tail(cloze[order(cloze$zClozeScore), ])

# Consider ClozeScore < 5 as outlier
threshold5 <- cloze[cloze$ClozeScore < 5, "Subject"]
# # Consider ClozeScore < 10 as outlier
threshold10 <- cloze[cloze$ClozeScore < 10, "Subject"]

## Exclude participants? (Uncomment to exclude)
# d <- d[!d$Subject %in% threshold5, ]
# d <- d[!d$Subject %in% threshold10, ]
```

### Data for GAMs comparing NS and L2 speakers

Subset data, define factors, set coding scheme, etc:

```{r}
## Single factor to model Group * Condition, using dummy coding:
# (The way to analyze crossed factors is to combine their levels into 
# a single factor, see Wood's comment here
# http://grokbase.com/t/r/r-help/113qaadxt4/r-how-to-add-in-interaction-terms-in-gamm)
d$GroupCondit <- with(d, interaction(Group, Condition))

## Subset data:
# Data for analysis of Path-priming vs Baseline
dp <- d %>% filter(Condition != 'Manner')
dp$Condition <- factor(dp$Condition) # drop unused levels
# Data for analysis of Manner-priming vs Baseline
dm <- d %>% filter(Condition != 'Path')
dm$Condition <- factor(dm$Condition) # drop unused levels

# Order levels correctly in each data frame
dp$GroupCondit <- factor(dp$GroupCondit, levels = c("NS.Baseline", "NS.Path",
                                                    "L2.Baseline", "L2.Path"))
dm$GroupCondit <- factor(dm$GroupCondit, levels = c("NS.Baseline", "NS.Manner",
                                                    "L2.Baseline", "L2.Manner"))
# note that NS Baseline becomes the reference group in both data frames
contrasts(dp$GroupCondit)
contrasts(dm$GroupCondit)
```


### Data for GAMs testing effect of proficiency in L2 speakers

Subset data, define factors, set coding scheme, etc:

```{r}
# subset L2 data only from Path/Baseline and Manner/Baseline conditions respectively
dp_l2 <- dp %>% filter(Group == "L2")
dm_l2 <- dm %>% filter(Group == "L2")
```


### Data for comparison of baseline conditions (mixed logit models)

Subset data, define factors, set coding scheme, etc:

```{r, echo = T}
# Subset data from baseline condition only
d_basel <- d %>% filter(Condition == "Baseline")
# And from baseline for L2 speakers only
d_basel_l2 <- d %>% filter(Condition == "Baseline" & Group == "L2")

# Use contrast coding to compare groups
contrasts(d_basel$Group) <- - contr.sum(2) / 2
colnames(contrasts(d_basel$Group)) <- "L2_vs_NS"
contrasts(d_basel$Group)
```



## Convenience functions etc.

[NB: might be hidden if `echo = FALSE`.]

```{r}
## Specify some global parameters

# adjust figure heght/width when not going with default (espec. for 2x2 plots)
myfighe_NS_L2 <- 6
myfighe_L2_prof <- 6
myfigwi <- 7
```


```{r}
## Source somewhat more complex functions

# source functions to compute collinearity diagnostics
source("functions/kappa_mer_fnc.R")
source("functions/vif_mer_fnc.R")

# Function that plots mixed model-estimates in logit space from baseline
# conditions, including speaker estimates
source("functions/plot_glmm_fnc.R")

# source multiplot function
source("functions/multiplot_fnc.R")

# Function used to load models if they have already been saved,
# rather than fitting them anew each time the script is called
source("functions/load_or_fit_fnc.R")

# Two functions to a) plot the differences between NS and L2 speakers from GAMs,
# and b) plot the effects by L2 speakers' proficiency from GAMs
source("functions/plot_gams_fnc.R")
```


```{r}
## Simpler convenience functions:

# print deviance explained as percentage
dev_expl <- function(fm) {
  devi <- summary(fm)$dev.expl
  paste0(round(100 * devi, 1), '% dev. explained')
}

# create a neat table of the summary of fixed effects of a mixed model
glmm_tb <- function(fm) {
  m <- round(summary(fm)$coefficients, 3)
  tb <- as.data.frame(m)
  names(tb) <- c("Estimate", "SE", "z-value", "p-value")
  kable(tb)
}

# save the document type being knited into variable
# cf. http://stackoverflow.com/questions/35144130/in-knitr-how-can-i-test-for-if-the-output-will-be-pdf-or-word/35149103
# needed for gam_tb() below
getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}
mydoctype <- getOutputFormat()
mydoctype

# create a neat table of the summary of GAMs
gam_tb <- function(fm = NULL, mytype = "html") {
  if (mydoctype == "html_document") {
    gamtabs(fm, type = "HTML")
  } else if (mydoctype == "pdf_document") {
    gamtabs(fm)
  } else {
    print("I don't know how to print a gam_table for this document type!")
  }
}
```

\begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
   \hline
A. parametric coefficients & Estimate & Std. Error & t-value & p-value \\ 
  (Intercept) & -1.1504 & 0.4822 & -2.3855 & 0.0171 \\ 
  GroupConditNS.Manner & 1.6219 & 0.6491 & 2.4985 & 0.0125 \\ 
  GroupConditL2.Baseline & -1.3624 & 0.6717 & -2.0282 & 0.0425 \\ 
  GroupConditL2.Manner & 1.5985 & 0.6532 & 2.4473 & 0.0144 \\ 
   \hline
B. smooth terms & edf & Ref.df & F-value & p-value \\ 
  s(Trial):GroupConditNS.Baseline & 1.0000 & 1.0001 & 8.0829 & 0.0045 \\ 
  s(Trial):GroupConditNS.Manner & 2.0502 & 2.5559 & 4.1008 & 0.2138 \\ 
  s(Trial):GroupConditL2.Baseline & 1.0000 & 1.0001 & 2.4500 & 0.1175 \\ 
  s(Trial):GroupConditL2.Manner & 1.5571 & 1.9194 & 10.5049 & 0.0102 \\ 
  s(Trial,Subject) & 114.5777 & 812.0000 & 849.3877 & $<$ 0.0001 \\ 
  s(VideoName) & 25.0025 & 31.0000 & 120.0840 & $<$ 0.0001 \\ 
   \hline
\end{tabular}
\caption{ } 
\label{tab.gam}
\end{table}

# Analyses and results


## Translation task

Descriptives for all three conditions (summary tables):


```{r}
# By condition and verb-type 
transl %>% group_by(Condition, Type) %>%
  summarise(ProportionCorrect= round(sum(Score) / n(), 2)) %>%
  kable

# By condition and verb
transl %>% group_by(Condition, Type, Target_verb) %>%
  summarise(ProportionCorrect= round(sum(Score) / n(), 2)) %>%
  kable
```

Simple by-subject plot for all three conditions:

```{r}
#  ------------------------------------------------------------------------
#  Descriptives (by-subject plots)
#  ------------------------------------------------------------------------

mywidth <- 7
myheight <- 3

# ggplot theme
mytheme <- theme_bw() + 
  theme(#text = element_text(size = 10),
    # panel.border = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line.x = element_line(color="black"),
    axis.line.y = element_line(color="black"))

by_subj <- transl %>% group_by(Subject, Condition, Type, ClozeScore) %>%
  summarise(Perc = round(sum(Score) / n(), 2))

ggplot(by_subj, aes(x = ClozeScore, y = Perc, colour = Type)) +
  geom_jitter(height = 0, alpha = .5) +
  facet_grid(. ~ Condition) +
  geom_smooth(method = "lm", se = FALSE) +
  ylab("Proportion of correct translations") +
  mytheme
```

Logit model of log-likelihood of correctly translating the meaning of the verb.

```{r}
#  ------------------------------------------------------------------------
#  Significance test (GLMM)
#  ------------------------------------------------------------------------

# It really makes most sense to focus on the control/baseline condition only
d_fm_baseline <- transl[transl$Condition == "Control", ]
# contrast coding for Type (Path vs Manner verb)
contrasts(d_fm_baseline$Type) <- contr.sum(2)
colnames(contrasts(d_fm_baseline$Type)) <- "MannerV_vs_PathV"
contrasts(d_fm_baseline$Type)
# center cloze scores
d_fm_baseline$cClozeScore <- as.vector(scale(d_fm_baseline$ClozeScore, scale = FALSE))

# Model treats the individual verbs as random effects
fm_transl <- glmer(Score ~ Type * cClozeScore + 
                     (1 | Subject) + (1 | Target_verb),
                   data = d_fm_baseline, family = "binomial")
summary(fm_transl)

fm_transl_eff <- allEffects(fm_transl)
plot(fm_transl_eff, type = "link", 
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-likelihood of correct\ntranslation of verb meaning",
     main = "Translation task")

```




## Baseline condition: NS vs L2ers

### Descriptives

```{r}
# # I had the following table initially to show baseline data only
# # But I think it's better to show descriptives for all the data.
# tb_baseline <- d_basel %>%
#   group_by(Group) %>%
#   summarise(N = n(),
#             P_V_n = sum(P_V),
#             P_V_perc = round(100 * sum(P_V) / n(), 1),
#             M_V_n = sum(M_V),
#             M_V_perc = round(100 * sum(M_V) / n(), 1))
# names(tb_baseline)[3:6] <- c("Path Vs (N)", "Path Vs (%)",
#                              "Manner Vs (N)", "Manner Vs (%)")
# kable(tb_baseline)
```


```{r}
# Descriptive data for all analysed conditions
# First need to convert data to long format:
d_long <- gather(d, VerbType, Used, P_V:M_V)
# table (will need to be appropriately formatted in report)
tb_descriptive <- d_long %>%
  group_by(VerbType, Condition, Group) %>%
  summarise(N = sum(Used),
            TotalN = n(),
            Percentage = round(100 * sum(Used) / n(), 1))
kable(tb_descriptive)
```



### Model for path-verb production

Use `glmer` to model the log-likelihood of using a path verb (`P_V`) in the baseline condition as a function of Group (L2 vs NS).
The model includes random intercepts by speaker and item (random slopes by item were justified by the data).

Model fit and summary of fixed effects:

```{r, echo = TRUE}
glmm_pve <- glmer(P_V ~ Group + (1 | Subject) + (1 + Group | VideoName),
                data = d_basel, family = "binomial", 
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
glmm_tb(glmm_pve)
```

Detailed summary of the model
```{r}
summary(glmm_pve)
```

Collinearity diagnostics:

```{r}
kappa.mer(glmm_pve)
vif.mer(glmm_pve)
```


### Model for manner-verb production

Use `glmer` to model the log-likelihood of using a manner verb (`M_V`) in the baseline condition as a function of Group (L2 vs NS).
The model includes random intercepts by speaker and item (adding random slopes by item does not significantly improve model fit, but we leave them in to keep the model maximal and identical to the path model).

Model fit and summary of fixed effects:

```{r, echo = TRUE}
glmm_mve <- glmer(M_V ~ Group + (1 | Subject) + (1 + Group | VideoName),
                data = d_basel, family = "binomial", 
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
glmm_tb(glmm_mve)
```

Detailed summary of the model:

```{r}
summary(glmm_mve)
```

Collinearity diagnostics:

```{r}
kappa.mer(glmm_mve)
vif.mer(glmm_mve)
```



### Plot model estimates


```{r, fig.width = myfigwi, fig.height = 3}
# save relevant plots
plot_basel_path <- plot_glmm(glmm_pve, d = d_basel, DV = "Path")
plot_basel_manner <- plot_glmm(glmm_mve, d = d_basel, DV = "Manner")
# combine them
multiplot(plot_basel_path, plot_basel_manner, cols = 2)
```


## Baseline condition: Effect of proficiency

Fit models predicting production of path-/manner-verbs as a function of proficiency.
Of course, we use only the L2 data.

### Path-verbs

```{r}
glmm_pve_prof <- glmer(P_V ~ cClozeScore + (1 | Subject) + (1 | VideoName),
                       data = d_basel_l2, family = "binomial", 
                       control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```


```{r}
summary(glmm_pve_prof)
```


### Manner-verbs

```{r}
glmm_mve_prof <- glmer(M_V ~ cClozeScore + (1 | Subject) + (1 | VideoName),
                       data = d_basel_l2, family = "binomial", 
                       control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```


```{r}
summary(glmm_mve_prof)
```

```{r, fig.width = myfigwi, fig.height = 3.5}
# I use this approach to plot model estimates for cClozeScore
# http://stats.stackexchange.com/questions/135255/obtaining-adjusted-predicted-proportions-with-lme4-using-the-glmer-function
# and using the following advice to combine them into a multiplot:
# http://stackoverflow.com/questions/15227184/combine-two-plots-created-with-effects-package-in-r
eff_p <- effect("cClozeScore", glmm_pve_prof)
eff_m <- effect("cClozeScore", glmm_mve_prof)

plot(eff_p, type = "link", ylim = myylims, 
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-likelihood\nof path verb",
     main = "Path verbs vs. proficiency",
     ticks.y = list(at = c(-2, 0)),
     rug = F, key.args=list(space="right"),
     row = 1,col = 1, nrow = 1,ncol = 2, 
     more = TRUE)
plot(eff_m, type = "link", ylim = myylims,
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-likelihood\nof manner verb",
     main = "Manner verbs vs. proficiency",
     rug=F, key.args=list(space="right"),
     row = 1,col = 2, nrow = 1, ncol = 2)
```





## Differences in adaptation over time (trial-by-trial analysis using GAMs)


### Path verbs in path-exposed vs control -- trial-by-trial

Previous models (not shown here) showed that random *intercepts* by items made sense, but not random slopes.
We add the former but not the latter.

Thus, the model is specified as:

`PathVerb ~ GroupCondit + s(Trial, by = GroupCondit) + s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're')`

Predictors are:

- `GroupCondit`: Group-Condition interaction as a fixed effect
- `s(Trial, by = GroupCondit)`: A smooth function of `Trial` allowing the function to differ for each level of Group-Condition (thin plate regression splines, the default)
- `s(Trial, Subject, bs = 'fs')`: Factor smooths for `Subject` to capture non-linear random effects of speakers
- `s(VideoName, bs = 're')`: Random intercepts by items (i.e., `VideoName`)


```{r}
# the expression that is passed to load_or_fit()
fm_p3.expr <- "bam(P_V ~ GroupCondit + s(Trial, by = GroupCondit) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = dp,
                  family = 'binomial')"
# load model or fit
load_or_fit("fm_p3", fm_p3.expr)
```

Summary of the model:

```{r}
summary(fm_p3)
```


Summary using gamtabs:


```{r, results = "asis"}
gam_tb(fm_p3)
```


Significance of the different terms in the model:

```{r}
anova(fm_p3)
```


Plot model estimates by condition, random smooth adjustments by speakers and QQ-plot of random by-item intercepts:

```{r, results = 'hide'}
par(mfrow = c(1, 1))
## plot
plot_smooth(fm_p3, view = 'Trial', plot_all = 'GroupCondit', rm.ranef=TRUE)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_p3, select = 5)
plot(fm_p3, select = 6)
```



Plot differences between conditions (model estimates):

```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_NS_L2(fm_p3, primed_cond = 'Path', mark.diff = TRUE)  # choose this on Linux machine
# plot_NS_L2(fm_p3, primed_cond = 'Path', mark.diff = FALSE)  # mark.diff is a hack to
# avoid an error that will stop the whole thing when I compile it on my work pc
# (it works fine on my own pc with linux)
```



### Manner verbs in Manner-exposed vs control -- trial-by-trial

Previous models (not shown here) showed that random *intercepts* by items made sense, but not random slopes.
We add the former but not the latter.

Thus, the model is specified as:

`MannerVerb ~ GroupCondit + s(Trial, by = GroupCondit) + s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're')`

Predictors are:

- `GroupCondit`: Group-Condition interaction as a fixed effect
- `s(Trial, by = GroupCondit)`: A smooth function of `Trial` allowing the function to differ for each level of Group-Condition (thin plate regression splines, the default)
- `s(Trial, Subject, bs = 'fs')`: Factor smooths for `Subject` to capture non-linear random effects of speakers
- `s(VideoName, bs = 're')`: Random intercepts by items (i.e., `VideoName`)


```{r}
# the expression that is passed to load_or_fit()
fm_m3.expr <- "bam(M_V ~ GroupCondit + s(Trial, by = GroupCondit) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = dm,
                  family = 'binomial')"
# load model or fit
load_or_fit("fm_m3", fm_m3.expr)
```

Summary of the model:

```{r}
summary(fm_m3)
```


Summary using gamtabs:

```{r, results = "asis"}
gam_tb(fm_m3)
```



Significance of the different terms in the model:

```{r}
anova(fm_m3)
```


Plot model estimates by condition, random smooth adjustments by speakers and QQ-plot of random by-item intercepts:

```{r, results = 'hide'}
## plot
plot_smooth(fm_m3, view = 'Trial', plot_all = 'GroupCondit', rm.ranef=TRUE)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m3, select = 5)
plot(fm_m3, select = 6)
```



Plot differences between conditions (model estimates):

```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_NS_L2(fm_m3, primed_cond = 'Manner')
```





## Changes in adaptation with increasing L2 proficiency


### Adaptation to path verbs vs proficiency

```{r}
fm_p_prof9.expr <- "bam(P_V ~ Condition +
                    te(Trial, ClozeScore, by = Condition) +
                    s(Trial, Subject, bs = 'fs') +
                    s(VideoName, bs = 're'),
                  data = dp_l2,
                  family = 'binomial')"
# load model or fit
load_or_fit("fm_p_prof9", fm_p_prof9.expr)
summary(fm_p_prof9)
```

Summary using gamtabs:

```{r, results = "asis"}
gam_tb(fm_p_prof9)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_p_prof9, select = 3)
plot(fm_p_prof9, select = 4)
```



```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_p_prof9, primed_cond = 'Path')
```


```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_p_prof9, view = c('Trial', 'ClozeScore'), 
           comp = list(Condition = c('Path', 'Baseline')),
           rm.ranef = TRUE)
```



### Adaptation to manner verbs vs proficiency

TO DO:

- Model comparison showing that 3-way interaction with proficiency significantly improves the model



```{r}
fm_m_prof9.expr <- "bam(M_V ~ Condition +
                    te(Trial, ClozeScore, by = Condition) +
                    s(Trial, Subject, bs = 'fs') +
                    s(VideoName, bs = 're'),
                  data = dm_l2,
                  family = 'binomial')"
# load model or fit
load_or_fit("fm_m_prof9", fm_m_prof9.expr)
summary(fm_m_prof9)
```

Summary using gamtabs:

```{r, results = "asis"}
gam_tb(fm_m_prof9)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m_prof9, select = 3)
plot(fm_m_prof9, select = 4)
```


```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_m_prof9, primed_cond = 'Manner')
```

```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_m_prof9, view = c('Trial', 'ClozeScore'), 
           comp = list(Condition = c('Manner', 'Baseline')), rm.ranef = TRUE)
```
