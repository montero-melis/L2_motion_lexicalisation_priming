---
title: "Analysis replication and supplementary information"
author: '[Guillermo Montero-Melis](http://www.biling.su.se/montero_melis_guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
  word_document:
    toc: yes
---



# Set up working space

## Libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 5)
```

```{r, include = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
library(dplyr)
packageVersion('dplyr')
library(tidyr)
packageVersion('tidyr')  # useful for gather() to convert from wide to long
library(mgcv)  # GAMs and GAMMs (Wood 2006)
packageVersion('mgcv')  # GAMs and GAMMs (Wood 2006)
library(itsadug)
packageVersion('itsadug')
library(lme4)
packageVersion('lme4')
library(ggplot2)
packageVersion('ggplot2')
library(boot)  # for inv.logit()
packageVersion('boot')  # for inv.logit()
library(knitr)  # for kable()
packageVersion('knitr')  # for kable()
library(lazyeval)  # lazy evaluation used in bysubj() function [summarise_]
packageVersion('lazyeval')
library(effects)
library(xtable)
```


## Data loading and processing

### Load data

```{r}
##  Load data
# The data is created in the script 'processing/compute_dependent_measures.R'
# There is the normal and the liberally coded version (see script for difference).
# Here I use the normal coding.

# load
d <- read.csv('../data/data_DVs.csv', fileEncoding = 'UTF-8', stringsAsFactors = TRUE)
# simplify somewhat
d <- d %>%
  select(Subject:VideoName, P_V, M_V) %>%
  rename(Trial = VideoTrial)
# head(d)
# str(d)

# Rename "Control" condition to "Baseline"
levels(d$Condition)[levels(d$Condition) == "Control"] <- "Baseline"

# participant data
ppts <- read.csv("../data/participants.csv", fileEncoding = "UTF-8",
                 stringsAsFactors = TRUE)
# head(ppts)
# participants by group
with(ppts, table(Group))
with(ppts, table(Group, Gender))
with(ppts, table(Group, Condition))
# age
ppts %>%
  group_by(Group) %>%
  summarise(Mage = mean(Age, na.rm = T), SDage = sd(Age, na.rm = T))
# Cloze scores
mean(ppts$ClozeScore, na.rm=T)
sd(ppts$ClozeScore, na.rm=T)

# No audio data recorded for Subject 14 (L2) due to experimental error; exclude
ppts <- ppts[ppts$Subject != 14, ]
# transform ClozeScore to z-score (as.vector prevents it from becoming a matrix)
ppts$zClozeScore <- as.vector(scale(ppts$ClozeScore))
# center ClozeScore but not scaling (as.vector prevents it from becoming a matrix)
ppts$cClozeScore <- as.vector(scale(ppts$ClozeScore, scale = FALSE))

# add speakers' clozescore to d:
d <- left_join(d, ppts %>% select(Subject, ClozeScore, zClozeScore, cClozeScore))

# Subject needs to be a factor
d$Subject <- factor(d$Subject)

# in Group, let the native speakers (NS) be the reference group
d$Group <- factor(d$Group, levels = c('NS', 'L2'))


# Add info about prime verbs to the data file (by joining two dataframes)
# Load table with priming verbs and then join
primes <- read.csv("../data/priming-verbs.csv")
head(primes)
d <- left_join(d, primes)


head(d)
str(d)
```

```{r}
# Data for translation task
transl <- read.csv("../data/L2_translation-task_scored.csv", fileEncoding = "UTF-8")
getwd()
```



### Remove outliers?

Remove any L2 speakers based on cloze score because too low?

```{r}
# Outliers? ---------------------------------------------------------------

# L2 speakers Cloze scores
cloze <- ppts %>%
  filter(Group == "L2") %>%
  select(Subject, Condition, ClozeScore, zClozeScore)
# Subjects ordered by clozescore
head(cloze[order(cloze$zClozeScore), ])
tail(cloze[order(cloze$zClozeScore), ])

# Consider ClozeScore < 5 as outlier
threshold5 <- cloze[cloze$ClozeScore < 5, "Subject"]
# # Consider ClozeScore < 10 as outlier
threshold10 <- cloze[cloze$ClozeScore < 10, "Subject"]

## Exclude participants? (Uncomment to exclude)
# d <- d[!d$Subject %in% threshold5, ]
# d <- d[!d$Subject %in% threshold10, ]
```

### Data for GAMs comparing NS and L2 speakers

- We will subset the data so that we can run one model for native speakers and another one for L2 speakers
- In each model we compare Baseline Path vs Exposed Path and Baseline Manner vs Exposed Manner

Below: Subset data, define factors, set coding scheme, etc.

```{r}
## Single factor to model VerbType * Condition, using dummy coding:
# (The way to analyze crossed factors is to combine their levels into 
# a single factor, see Wood's comment here
# http://grokbase.com/t/r/r-help/113qaadxt4/r-how-to-add-in-interaction-terms-in-gamm)

# First need to convert data to long format:
d_long <- gather(d, VerbType, Used, P_V:M_V)
# single factor
d_long$VbType_Cond <- with(d_long, interaction(VerbType, Condition))

# For the subjects  to be properly fitted as random smooths in the GAMs
# we have to "pretend" that a baseline subject is two different subjects,
# one for the comparison to path-primed, the other to manner-primed participants;
# this may not be ideal statistically, but not doing this would turn the random
# smooths into random intercepts, which is much worse.
d_long$Subject <- with(d_long, interaction(Subject, VerbType))

## Subset data:
# Data for analysis of Native speakers (note we are removing observations that
# correspond to path verbs produced in the manner-primed condition or to manner
# verbs produced in the path-primed condition)
d_ns <- d_long %>% filter(Group == "NS" & ! VbType_Cond %in% c("P_V.Manner", "M_V.Path"))
# drop unused factors for subject (I think this is important for gam fitting)
d_ns$Subject <- factor(d_ns$Subject)
# Data for analysis of L2 speakers
d_l2 <- d_long %>% filter(Group == "L2" & ! VbType_Cond %in% c("P_V.Manner", "M_V.Path"))
d_l2$Subject <- factor(d_l2$Subject)

# Order levels correctly in each data frame
d_ns$VbType_Cond <- factor(d_ns$VbType_Cond, levels = c("P_V.Baseline", "P_V.Path",
                                                      "M_V.Baseline", "M_V.Manner"))
d_l2$VbType_Cond <- factor(d_l2$VbType_Cond, levels = c("P_V.Baseline", "P_V.Path",
                                                      "M_V.Baseline", "M_V.Manner"))
# note that log-odds of path verbs in baseline condition becomes the reference
# level in both groups (NS and L2 speakers)
contrasts(d_ns$VbType_Cond)
contrasts(d_l2$VbType_Cond)
```


### Data for GAMs testing effect of proficiency in L2 speakers

Subset data, define factors, set coding scheme, etc:

```{r}
# subset L2 data only from Path/Baseline and Manner/Baseline conditions respectively
dp_l2 <- d_l2 %>% filter(VerbType == "P_V")
dm_l2 <- d_l2 %>% filter(VerbType == "M_V")
```


### Data for comparison of baseline conditions (mixed logit models)

Subset data, define factors, set coding scheme, etc:

```{r, echo = T}
# Subset data from baseline condition only
d_basel <- d %>% filter(Condition == "Baseline")
# And from baseline for L2 speakers only
d_basel_l2 <- d %>% filter(Condition == "Baseline" & Group == "L2")

# Use contrast coding to compare groups
contrasts(d_basel$Group) <- - contr.sum(2) / 2
colnames(contrasts(d_basel$Group)) <- "L2_vs_NS"
contrasts(d_basel$Group)
```



## Convenience functions etc.

[NB: might be hidden if `echo = FALSE`.]

```{r}
## Specify some global parameters

# adjust figure heght/width when not going with default (espec. for 2x2 plots)
myfighe_NS_L2 <- 6
myfighe_L2_prof <- 6
myfigwi <- 7
```


```{r}
## Source somewhat more complex functions

# source functions to compute collinearity diagnostics
source("functions/kappa_mer_fnc.R")
source("functions/vif_mer_fnc.R")

# Function that plots mixed model-estimates in logit space from baseline
# conditions, including speaker estimates
source("functions/plot_glmm_fnc.R")

# source multiplot function
source("functions/multiplot_fnc.R")

# Function used to load models if they have already been saved,
# rather than fitting them anew each time the script is called
source("functions/load_or_fit_fnc.R")

# Two functions to a) plot the differences between NS and L2 speakers from GAMMs,
# and b) plot the effects by L2 speakers' proficiency from GAMMs
source("functions/plot_gams_fnc.R")
```


```{r}
## Simpler convenience functions:

# print deviance explained as percentage
dev_expl <- function(fm) {
  devi <- summary(fm)$dev.expl
  paste0(round(100 * devi, 1), '% dev. explained')
}

# create a neat table of the summary of fixed effects of a mixed model
glmm_tb <- function(fm) {
  m <- round(summary(fm)$coefficients, 3)
  tb <- as.data.frame(m)
  names(tb) <- c("Estimate", "SE", "z-value", "p-value")
  kable(tb)
}

# create a neat table of the summary of GAMs
gam_tb <- function(fm = NULL, mytype = "html") {
  if (mydoctype == "html_document") {
    gamtabs(fm, type = "HTML")
  } else if (mydoctype == "pdf_document") {
    gamtabs(fm)
  } else {
    print("I don't know how to print a gam_table for this document type!")
  }
}
```

```{r}
# save the document type being knited into variable
# cf. http://stackoverflow.com/questions/35144130/in-knitr-how-can-i-test-for-if-the-output-will-be-pdf-or-word/35149103
# needed for gam_tb() below
getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}
# create a default to avoid error call when sourcing script without knitting
mydoctype <- 0

# NB: uncomment following lines before knitting the document!

mydoctype <- getOutputFormat()
# NB: this function call will crash if the document is not being knitted, but
# simply sourced from the R session
mydoctype
```




# Analyses and results


## Translation task

Descriptives for all three conditions (summary tables):


```{r}
# By condition and verb-type 
transl %>% group_by(Condition, Type) %>%
  summarise(ProportionCorrect= round(sum(Score) / n(), 2)) %>%
  kable

# By condition and verb
transl %>% group_by(Condition, Type, Target_verb) %>%
  summarise(ProportionCorrect= round(sum(Score) / n(), 2)) %>%
  kable
```

Simple by-subject plot for all three conditions:

```{r}
#  ------------------------------------------------------------------------
#  Descriptives (by-subject plots)
#  ------------------------------------------------------------------------

mywidth <- 7
myheight <- 3

# ggplot theme
mytheme <- theme_bw() + 
  theme(#text = element_text(size = 10),
    # panel.border = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line.x = element_line(color="black"),
    axis.line.y = element_line(color="black"))

by_subj <- transl %>% group_by(Subject, Condition, Type, ClozeScore) %>%
  summarise(Perc = round(sum(Score) / n(), 2))

ggplot(by_subj, aes(x = ClozeScore, y = Perc, colour = Type)) +
  geom_jitter(height = 0, alpha = .5) +
  facet_grid(. ~ Condition) +
  geom_smooth(method = "lm", se = FALSE) +
  ylab("Proportion of correct translations") +
  mytheme
```

Logit model of log-odds of correctly translating the meaning of the verb.

```{r fit_glmm_translation_data}
#  ------------------------------------------------------------------------
#  Significance test (GLMM)
#  ------------------------------------------------------------------------

# It really makes most sense to focus on the control/baseline condition only
d_fm_baseline <- transl[transl$Condition == "Control", ]
# contrast coding for Type (Path vs Manner verb)
contrasts(d_fm_baseline$Type) <- contr.sum(2)
colnames(contrasts(d_fm_baseline$Type)) <- "MannerV_vs_PathV"
contrasts(d_fm_baseline$Type)
# center cloze scores
d_fm_baseline$cClozeScore <- as.vector(scale(d_fm_baseline$ClozeScore, scale = FALSE))

# Model treats the individual verbs as random effects
fm_transl <- glmer(Score ~ Type * cClozeScore + 
                     (1 | Subject) + (1 | Target_verb),
                   data = d_fm_baseline, family = "binomial")
summary(fm_transl)

fm_transl_eff <- allEffects(fm_transl)
plot(fm_transl_eff, type = "link", 
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-odds of correct\ntranslation of verb meaning",
     main = "Translation task")

```



## Baseline condition: NS vs L2ers

### Descriptives

```{r}
# Descriptive data for all analysed conditions
# table (will need to be appropriately formatted in report)
tb_descriptive <- d_long %>%
  group_by(VerbType, Condition, Group) %>%
  summarise(N = sum(Used),
            TotalN = n(),
            Percentage = round(100 * sum(Used) / n(), 1))
kable(tb_descriptive)
```



### Model for path-verb production

Use `glmer` to model the log-odds of using a path verb (`P_V`) in the baseline condition as a function of Group (L2 vs NS).
The model includes random intercepts by speaker and item (random slopes by item were justified by the data).

Model fit and summary of fixed effects:

```{r, echo = TRUE}
glmm_pve <- glmer(P_V ~ Group + (1 | Subject) + (1 + Group | VideoName),
                data = d_basel, family = "binomial", 
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
glmm_tb(glmm_pve)
```

Detailed summary of the model
```{r}
summary(glmm_pve)
```

Collinearity diagnostics:

```{r}
kappa.mer(glmm_pve)
vif.mer(glmm_pve)
```


### Model for manner-verb production

Use `glmer` to model the log-odds of using a manner verb (`M_V`) in the baseline condition as a function of Group (L2 vs NS).
The model includes random intercepts by speaker and item (adding random slopes by item does not significantly improve model fit, but we leave them in to keep the model maximal and identical to the path model).

Model fit and summary of fixed effects:

```{r, echo = TRUE}
glmm_mve <- glmer(M_V ~ Group + (1 | Subject) + (1 + Group | VideoName),
                data = d_basel, family = "binomial", 
                control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
glmm_tb(glmm_mve)
```

Detailed summary of the model:

```{r}
summary(glmm_mve)
```

Collinearity diagnostics:

```{r}
kappa.mer(glmm_mve)
vif.mer(glmm_mve)
```



### Plot model estimates


```{r, fig.width = myfigwi, fig.height = 3}
# save relevant plots
plot_basel_path <- plot_glmm(glmm_pve, d = d_basel, DV = "Path")
plot_basel_manner <- plot_glmm(glmm_mve, d = d_basel, DV = "Manner")
# combine them
multiplot(plot_basel_path, plot_basel_manner, cols = 2)
```


## Baseline condition: Effect of proficiency

Fit models predicting production of path-/manner-verbs as a function of proficiency.
Of course, we use only the L2 data.

### Path-verbs

```{r}
glmm_pve_prof <- glmer(P_V ~ cClozeScore + (1 | Subject) + (1 | VideoName),
                       data = d_basel_l2, family = "binomial", 
                       control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```


```{r}
summary(glmm_pve_prof)
```

As L2 speakers in the baseline condition become more proficient, they become more likely to produce path verbs.


### Manner-verbs

```{r}
glmm_mve_prof <- glmer(M_V ~ cClozeScore + (1 | Subject) + (1 | VideoName),
                       data = d_basel_l2, family = "binomial", 
                       control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
```


```{r}
summary(glmm_mve_prof)
```

However, L2 speakers in the baseline condition do not change their likelihood of producing manner verbs as a function of proficiency.
 


```{r, fig.width = myfigwi, fig.height = 3.5}
# I use this approach to plot model estimates for cClozeScore
# http://stats.stackexchange.com/questions/135255/obtaining-adjusted-predicted-proportions-with-lme4-using-the-glmer-function
# and using the following advice to combine them into a multiplot:
# http://stackoverflow.com/questions/15227184/combine-two-plots-created-with-effects-package-in-r
eff_p <- effect("cClozeScore", glmm_pve_prof)
eff_m <- effect("cClozeScore", glmm_mve_prof)

plot(eff_p, type = "link", ylim = myylims, 
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-odds\nof path verb",
     main = "Path verbs vs. proficiency",
     ticks.y = list(at = c(-2, 0)),
     rug = F, key.args=list(space="right"),
     row = 1,col = 1, nrow = 1,ncol = 2, 
     more = TRUE)
plot(eff_m, type = "link", ylim = myylims,
     xlab = "Proficiency\n(centred cloze score)",
     ylab = "Log-odds\nof manner verb",
     main = "Manner verbs vs. proficiency",
     rug=F, key.args=list(space="right"),
     row = 1,col = 2, nrow = 1, ncol = 2)
```





## Differences in adaptation over time (trial-by-trial analysis using GAMs)


### Native speakers -- trial-by-trial

Previous models (not shown here) showed that random *intercepts* by items made sense, but not random slopes.
We add the former but not the latter.

Thus, the model is specified as:

`PathVerb ~ VbType_Cond + s(Trial, by = VbType_Cond) + s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're')`

Predictors are:

- `VbType_Cond`: Group-Condition interaction as a fixed effect
- `s(Trial, by = VbType_Cond)`: A smooth function of `Trial` allowing the function to differ for each level of Group-Condition (thin plate regression splines, the default)
- `s(Trial, Subject, bs = 'fs')`: Factor smooths for `Subject` to capture non-linear random effects of speakers
- `s(VideoName, bs = 're')`: Random intercepts by items (i.e., `VideoName`)


```{r gam_ns}
# the expression that is passed to load_or_fit()
gam_ns.expr <- "bam(Used ~ VbType_Cond + s(Trial, by = VbType_Cond) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = d_ns,
                  family = 'binomial')"
# load model or fit
load_or_fit("gam_ns", gam_ns.expr)
```

Summary of the model:

```{r}
summary(gam_ns)
```


Summary using gamtabs:


```{r, results = "asis"}
gam_tb(gam_ns)
```


Significance of the different terms in the model:

```{r}
anova(gam_ns)
```


Plot model estimates by condition, random smooth adjustments by speakers and QQ-plot of random by-item intercepts:

```{r, results = 'hide'}
## plot
plot_smooth(gam_ns, view = 'Trial', plot_all = 'VbType_Cond', rm.ranef=TRUE)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(gam_ns, select = 5)
plot(gam_ns, select = 6)
```



Plot differences between conditions (model estimates):

```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_gam_main(gam_ns, "NS")  # choose this on Linux machine
# on my old windows computer:
# plot_gam_main(gam_ns, "NS", mark.diff = FALSE)  # use of mark.diff is a hack to
# avoid an error that will stop the whole thing when I compile it on my work pc
```



### L2-speaker adaptation -- trial-by-trial

Previous models (not shown here) showed that random *intercepts* by items made sense, but not random slopes.
We add the former but not the latter.

Thus, the model is specified as:
  
  `MannerVerb ~ VbType_Cond + s(Trial, by = VbType_Cond) + s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're')`

Predictors are:
  
  - `VbType_Cond`: Group-Condition interaction as a fixed effect
- `s(Trial, by = VbType_Cond)`: A smooth function of `Trial` allowing the function to differ for each level of Group-Condition (thin plate regression splines, the default)
- `s(Trial, Subject, bs = 'fs')`: Factor smooths for `Subject` to capture non-linear random effects of speakers
- `s(VideoName, bs = 're')`: Random intercepts by items (i.e., `VideoName`)


```{r gam_l2}
# the expression that is passed to load_or_fit()
gam_l2.expr <- "bam(Used ~ VbType_Cond + s(Trial, by = VbType_Cond) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = d_l2,
                  family = 'binomial')"
# load model or fit
load_or_fit("gam_l2", gam_l2.expr)
```

Summary of the model:
  
```{r}
summary(gam_l2)
```


Summary using gamtabs:
  
```{r, results = "asis"}
gam_tb(gam_l2)
```



Significance of the different terms in the model:
  
```{r}
anova(gam_l2)
```


Plot model estimates by condition, random smooth adjustments by speakers and QQ-plot of random by-item intercepts:
  
```{r, results = 'hide'}
## plot
plot_smooth(gam_l2, view = 'Trial', plot_all = 'VbType_Cond', rm.ranef=TRUE)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(gam_l2, select = 5)
plot(gam_l2, select = 6)
```



Plot differences between conditions (model estimates):
  
```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_gam_main(gam_l2, "L2")
```



### L2-speaker adaptation -- beginning of experiment

We now model only the linear part of the curve for Path and Manner, so that we can run a mixed logit model and test for differences in slopes between path and manner conditions.

```{r}
# linear part corresponds to trials 1-9
d_l2_beg <- d_l2 %>% filter(Trial %in% 1:9)
# This time we want to analyze it as a 2 x 2 design: 
# VerbType (Path vs Manner) x Condition (Exposed vs Baseline)
# The two factors interact with themselves and with Trial (centred)

# VerbType -- use contrast coding
d_l2_beg$VerbType <- factor(d_l2_beg$VerbType)
contrasts(d_l2_beg$VerbType) <- - contr.sum(2) / 2
colnames(contrasts(d_l2_beg$VerbType)) <- "P_vs_M"
contrasts(d_l2_beg$VerbType)

# Condition (now becomes a binary variable: Path/Manner become "Primed")
levels(d_l2_beg$Condition)[levels(d_l2_beg$Condition) %in% c("Path", "Manner")] <- "Primed"
levels(d_l2_beg$Condition)
table(d_l2_beg$Condition)  # roughly balanced
# contrast coding
contrasts(d_l2_beg$Condition) <- - contr.sum(2) / 2
colnames(contrasts(d_l2_beg$Condition)) <- "Primed_vs_Baseline"
contrasts(d_l2_beg$Condition)

# Centre Trial
d_l2_beg$cTrial <- d_l2_beg$Trial - mean(d_l2_beg$Trial)

# Note that some items (i.e. VideoName) might now be missing
length(unique(d_l2_beg$VideoName))  # but they aren't
sort(table(d_l2_beg$VideoName))
# broken down by verbtype...
with(d_l2_beg, table(VerbType, VideoName))
# ... and by condition
with(d_l2_beg, table(Condition, VideoName))


head(d_l2_beg)
```

We can now fit the logit mixed model

```{r fit_glmm_beginning}
# The following snippet fitted different models to analyze the first trials;
# in the end we choose the more conservative one that still converges 
# -- here glmm_adapt_beg3b; this is the one that is fitted below and reported.

# # minimal random effects
# glmm_adapt_beg_min <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 | Subject) + (1 | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg_min)

# progressively adding r.e. ...

# # 1 random slope
# glmm_adapt_beg1a <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 + cTrial | Subject) + (1 | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg1a)
# 
# glmm_adapt_beg1b <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 | Subject) + (1 + Condition | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg1b)
# 
# # fails to converge:
# # glmm_adapt_beg1c <- glmer(Used ~ Condition * VerbType * cTrial +
# #                           (1 | Subject) + (1 + VerbType | VideoName),
# #                         data = d_l2_beg, family = "binomial",
# #                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# # summary(glmm_adapt_beg1c)
# 
# # 2 random slopes
# glmm_adapt_beg2a <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 + cTrial | Subject) + (1 + Condition | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg2a)
# 
# # fails to converge:
# # glmm_adapt_beg2b <- glmer(Used ~ Condition * VerbType * cTrial +
# #                           (1 + cTrial | Subject) + (1 + VerbType | VideoName),
# #                         data = d_l2_beg, family = "binomial",
# #                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# # summary(glmm_adapt_beg2b)

# glmm_adapt_beg2c <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 | Subject) + (1 + Condition + VerbType | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg2c)

# 3 random slopes

# fails to converge:
# glmm_adapt_beg3a <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 + cTrial | Subject) + (1 + Condition + VerbType | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg3a)

# glmm_adapt_beg3b <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 | Subject) + (1 + Condition * VerbType | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg3b)


# maximal random effects

# fails to converge
# glmm_adapt_beg_max <- glmer(Used ~ Condition * VerbType * cTrial +
#                           (1 + cTrial | Subject) + (1 + Condition * VerbType | VideoName),
#                         data = d_l2_beg, family = "binomial",
#                         control = glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# summary(glmm_adapt_beg_max)

# # compare all converging models
# anova(glmm_adapt_beg_min, glmm_adapt_beg1a, glmm_adapt_beg1b, glmm_adapt_beg2a, 
#       glmm_adapt_beg2c, glmm_adapt_beg3b)

# # compare the one that seems to contribute most with the null
# anova(glmm_adapt_beg_min, glmm_adapt_beg2c, glmm_adapt_beg3b)

# summary(glmm_adapt_beg2c)
# summary(glmm_adapt_beg3b)

# In the end we choose the more conservative model: glmm_adapt_beg3b
```


```{r}
# The final model to analyze adaptation to path vs manner verbs during the
# beginning (=linear in log-odds space) phase of the experiment
# the expression that is passed to load_or_fit()
glmm_adapt_beg.expr <- "glmer(Used ~ Condition * VerbType * cTrial +
                          (1 | Subject) + (1 + Condition * VerbType | VideoName),
                        data = d_l2_beg, family = 'binomial',
                        control = glmerControl(optimizer='bobyqa', optCtrl=list(maxfun=2e5)))"
# load model or fit
load_or_fit("glmm_adapt_beg", glmm_adapt_beg.expr)
```



Plot 

```{r fig.height = myfighe_NS_L2, fig.width = myfigwi}
plot(allEffects(glmm_adapt_beg))
```




## Changes in adaptation with increasing L2 proficiency


### Adaptation to path verbs vs proficiency

```{r}
fm_p_L2prof.expr <- "bam(Used ~
    Condition +
    te(Trial, ClozeScore, by = Condition) +
    s(Trial, Subject, bs = 'fs') +
    s(VideoName, bs = 're'),
  data = dp_l2,
  family = 'binomial')"
# load model or fit
load_or_fit("fm_p_L2prof", fm_p_L2prof.expr)
summary(fm_p_L2prof)
```

Summary using gamtabs:
  
```{r, results = "asis"}
gam_tb(fm_p_L2prof)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_p_L2prof, select = 3)
plot(fm_p_L2prof, select = 4)
```



```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_p_L2prof, primed_cond = 'Path')
```


```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_p_L2prof, view = c('Trial', 'ClozeScore'),
           comp = list(Condition = c('Path', 'Baseline')),
           rm.ranef = TRUE)
```



### Adaptation to manner verbs vs proficiency

TO DO:
  
- Model comparison showing that 3-way interaction with proficiency significantly improves the model



```{r}
fm_m_L2prof.expr <- "bam(Used ~ Condition +
te(Trial, ClozeScore, by = Condition) +
s(Trial, Subject, bs = 'fs') +
s(VideoName, bs = 're'),
data = dm_l2,
family = 'binomial')"
# load model or fit
load_or_fit("fm_m_L2prof", fm_m_L2prof.expr)
summary(fm_m_L2prof)
```

Summary using gamtabs:
  
```{r, results = "asis"}
gam_tb(fm_m_L2prof)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m_L2prof, select = 3)
plot(fm_m_L2prof, select = 4)
```


```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_m_L2prof, primed_cond = 'Manner')
```

```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_m_L2prof, view = c('Trial', 'ClozeScore'),
           comp = list(Condition = c('Manner', 'Baseline')), rm.ranef = TRUE)
```



## Control analyses for L2 speakers

For the L2 speakers, there is a possibility that adaptation is not guided by their expectations (as we argue in the paper), but by their lexical knowledge of the L2 verbs.
For example, it might  be that they are primed only by the specific verbs they know, but not by those they do not know.
To explore this possibility, we run the same analyses removing the two manner verbs that were less well known to L2 speakers in the baseline condition: *arrastrar* (=drag, 65% translation accuracy) and *tirar de* (=pull, 55% transl. acc.).



### L2 speaker adaptation trial by trial (without proficiency in the model)

#### Without *tirar*

First a model excluding the least well known manner verb: *tirar de*.

```{r}
# Run the model as in main analysis, but with L2 data excluding items primed
# with "tira de" (applies to subjects in Manner condition only); see 'data'
# argument in call below
# the expression that is passed to load_or_fit()
gam_l2_notira.expr <- "bam(Used ~ VbType_Cond + s(Trial, by = VbType_Cond) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = d_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV == 'tira de')),
                  family = 'binomial')"
# load model or fit
load_or_fit("gam_l2_notira", gam_l2_notira.expr)
```

Summary of the model:
  
```{r}
summary(gam_l2_notira)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(gam_l2_notira, select = 5)
plot(gam_l2_notira, select = 6)
```


Plot differences between conditions (model estimates):
  
```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_gam_main(gam_l2_notira, "L2")
```




#### Without *arrastrar*

Then a model excluding the 2nd least well known manner verb: *arrastrar*.


```{r}
# the expression that is passed to load_or_fit()
# note data argument in expression below to subset right data
gam_l2_noarrastra.expr <- "bam(Used ~ VbType_Cond + s(Trial, by = VbType_Cond) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = d_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV == 'arrastra')),
                  family = 'binomial')"
# load model or fit
load_or_fit("gam_l2_noarrastra", gam_l2_noarrastra.expr)
```


Summary of the model:
  
```{r}
summary(gam_l2_noarrastra)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(gam_l2_noarrastra, select = 5)
plot(gam_l2_noarrastra, select = 6)
```


Plot differences between conditions (model estimates):
  
```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_gam_main(gam_l2_noarrastra, "L2")
```



#### Without either *tirar* or *arrastrar*

Finally a model that excludes both manner verbs not well known.


```{r}
# the expression that is passed to load_or_fit()
# Note subset of data passed to 'data' argument
gam_l2_no_tira_arrastra.expr <- "bam(Used ~ VbType_Cond + s(Trial, by = VbType_Cond) +
                    s(Trial, Subject, bs = 'fs') + s(VideoName, bs = 're'),
                  data = d_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV %in% c('tira de', 'arrastra'))),
                  family = 'binomial')"
# load model or fit
load_or_fit("gam_l2_no_tira_arrastra", gam_l2_no_tira_arrastra.expr)
```



Summary of the model:
  
```{r}
summary(gam_l2_no_tira_arrastra)
```



```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(gam_l2_no_tira_arrastra, select = 5)
plot(gam_l2_no_tira_arrastra, select = 6)
```


Plot differences between conditions (model estimates):
  
```{r, fig.height = myfighe_NS_L2, fig.width = myfigwi, echo = FALSE, results = 'hide'}
plot_gam_main(gam_l2_no_tira_arrastra, "L2")
```



### Manner adaptation with L2 proficiency in model


#### Without *tira de*


```{r}
fm_m_L2prof_notira.expr <- "bam(Used ~ Condition +
te(Trial, ClozeScore, by = Condition) +
s(Trial, Subject, bs = 'fs') +
s(VideoName, bs = 're'),
data = dm_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV == 'tira de')),
family = 'binomial')"
# load model or fit
load_or_fit("fm_m_L2prof_notira", fm_m_L2prof_notira.expr)
```

```{r}
summary(fm_m_L2prof_notira)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m_L2prof_notira, select = 3)
plot(fm_m_L2prof_notira, select = 4)
```


```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_m_L2prof_notira, primed_cond = 'Manner')
```


```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_m_L2prof_notira, view = c('Trial', 'ClozeScore'),
           comp = list(Condition = c('Manner', 'Baseline')), rm.ranef = TRUE)
```




#### Without *arrastra*


```{r}
fm_m_L2prof_noarrastra.expr <- "bam(Used ~ Condition +
te(Trial, ClozeScore, by = Condition) +
s(Trial, Subject, bs = 'fs') +
s(VideoName, bs = 're'),
data = dm_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV == 'arrastra')),
family = 'binomial')"
# load model or fit
load_or_fit("fm_m_L2prof_noarrastra", fm_m_L2prof_noarrastra.expr)
```

```{r}
summary(fm_m_L2prof_noarrastra)
```

```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m_L2prof_noarrastra, select = 3)
plot(fm_m_L2prof_noarrastra, select = 4)
```

```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_m_L2prof_noarrastra, primed_cond = 'Manner')
```


```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_m_L2prof_noarrastra, view = c('Trial', 'ClozeScore'),
           comp = list(Condition = c('Manner', 'Baseline')), rm.ranef = TRUE)
```



#### Without either *tira de* or *arrastra*


```{r}
fm_m_L2prof_no_tira_arrastra.expr <- "bam(Used ~ Condition +
te(Trial, ClozeScore, by = Condition) +
s(Trial, Subject, bs = 'fs') +
s(VideoName, bs = 're'),
data = dm_l2 %>% filter(!(Condition == 'Manner' & MannerPrimeV %in% c('tira de', 'arrastra'))),
family = 'binomial')"
# load model or fit
load_or_fit("fm_m_L2prof_no_tira_arrastra", fm_m_L2prof_no_tira_arrastra.expr)
```

```{r}
summary(fm_m_L2prof_no_tira_arrastra)
```


```{r}
# show by-speaker random smooths and by-item random intercepts
par(mfrow = c(1, 2))
plot(fm_m_L2prof_no_tira_arrastra, select = 3)
plot(fm_m_L2prof_no_tira_arrastra, select = 4)
```


```{r, fig.height = myfighe_L2_prof, fig.width = myfigwi, results='hide'}
plot_L2_profic(fm_m_L2prof_no_tira_arrastra, primed_cond = 'Manner')
```


```{r, results='hide'}
# difference plot in 3D
plot_diff2(fm_m_L2prof_no_tira_arrastra, view = c('Trial', 'ClozeScore'),
           comp = list(Condition = c('Manner', 'Baseline')), rm.ranef = TRUE)
```



