## Run follow-up analyses on Q3, based on suggestion by TFJ to assess the
## likelihood of the L2ers' data under the native speaker model.


# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Conceptual background (email correspondence) ----------------------------
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

## I) Initial proposal

## TFJ's email (fre 2018-09-14 22:09 "Re: testing for interactions in GAMs"):

# There's one idea I had just now that I kinda like that we can implement after
# submission. It's very easy to implement:
#   
# i) take a model of the natives that you already have (this could be done
# while considering trial, i.e., the model from question 1, or without trial,
# i.e., the baseline model). 
# 
# ii) then use the predict function of the gamm to predict the data of all
# non-native participants (i.e., new data with new individuals). Store the
# predicted probability of the actually observed outcome of each trial (either
# for the joint analysis or for both of the separate path verb? + manner verb?
# analyses). 
# 
# iii) log the probability of each trial and then sum these log-probabilities
# by-participant, giving you log-likelihoods (LL) of each non-native participant
# being generated by the native model. I.e., 
# 
# data of non-natives$predicted = predict(model of native, newdata = data of non-natives)
# 
# data of non-natives$ll = log(ifelse(actual outcome == 1, predicted, 1-predicted))
#
# data of non-native %>%
#   group_by(Subject, Priming condition) %>%
#   summarise(ll = mean(ll))
#                              
# iv) plot these LLs (y-axis) against proficiency (x-axis), separated by
# priming condition (3 colors). fit 3 (color) or 1 (joint) smoother through
# this to see whether LL increases with proficiency.
# 
# v) if we so fancy, one can analyze the LLs with a gamm or glmm, but perhaps
# the visualization does the job?


## II) Problem

## The procedure suggested above does not work quite at first, as I explain in
## an email (Mon 2018-09-17 17:53 "Re: testing for interactions in GAMs"):

# Regarding your last suggestion for the follow-up on Q3 (comparison of L2ers
# as a function of proficiency and natives using the predict function), the
# following line of code:
#   
# gam_ns_fit_to_L2 <- predict.gam(gam_ns, newdata = d_l2, type = "response")
#   
# yields the error:
# Error in predict.gam(gam_ns, newdata = d_l2, type = "response") : 
#     1.M_V, 10.M_V, 11.M_V, 12.M_V, ... 9.P_V not in original fit
#   
# predict.gam() complains because the L2 subjects are not the same as the 
# natives. This is understandable, since the original model we fit 
# [ Used ~ VbType_Cond + s(Trial, by = VbType_Cond) + s(Trial, Subject, bs = "fs")
# + s(VideoName, bs = "re") ] 
# estimates factor smooths for each (native) subject.
#   
# It seems like the lme4 version of predict can handle such a scenario by simply
# omitting the random effects when predicting (using the argument “re.form = NA”,
# see this thread), but the predict.gam doesn’t. I tried to follow what I believe
# was suggested in the thread, by fitting the same model for native speakers but
# with the data:
#
# d_ns_all_ppt_levels <- d_ns
# levels(d_ns_all_ppt_levels$Subject) <- c(levels(d_ns$Subject), levels(d_l2$Subject))
# 
# That is, including the levels of the L2ers in the Subject factor, but I could
# not fool gam with this cheap trick.


## III) Solution

## TFJ's email (Tue tis 2018-09-18 22:33 "Re: testing for interactions in GAMs"):

# Luckily, there's a reasonable 'simple' solution:
# 
# 1) make a copy of your non-native data
# 
# 2) override the subject info by randomly assigning native subject IDs, say
# with replacement.
# 
# copy_nonnative_data %<>%
# group_by(Subject) %>%
# mutate(Subject = sample(levels(native_data$Subject), 1, replace = T))
# 
# 3) Do the predict as previously discussed. Get the LLs for each subject.
# Remember them in e.g., a table or data.frame with one row per subject and
# columns for proficiency, LL, and prime condition.
# 
# 4) Repeat step 1-3 about 200 times, each time appending the the new rows to
# the data.frame/table from step 3. 
# 
# 5) Make the same plot (x = profiency, y = LL, color = priming condition)
# as discussed before. This is easy and has the nice side benefit that each
# non-native subject now forms a distribution (with CIs, yay!) of LLs.
# 
# The simplifying assumption behind this is that we're assessing non-native
# subject's 'nativeness' (likelihood) while assuming that the sample of random
# differences among natives is sufficiently representative of the population.
# I think that's ok (but the CIs will also give you an idea how much of a
# non-native  subject's 'nativeness' depends on the random by-subject variances).


# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Set up workspace --------------------------------------------------------
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

library(dplyr)
# library(lme4)
# library(tidyr)
library(ggplot2)
library(mgcv)  # GAMs and GAMMs (Wood 2006)
# library(itsadug)
# library(boot)  # for inv.logit()	
# library(lazyeval)  # lazy evaluation used in bysubj() function [summarise_]	
# library(effects)	
# library(xtable)	


## Load data files
d <- read.csv("data/data_gamms_baseline-doublecounted.csv",
              stringsAsFactors = FALSE)
head(d)

# divide into data from native speakers (NS) and L2ers
d_ns <- d %>% filter(Group == "NS")
d_ns$Subject <- factor(d_ns$Subject)  # drop unused factors for subject
d_l2 <- d %>% filter(Group == "L2")
d_l2$Subject <- factor(d_l2$Subject)  # drop unused factors for subject

# load native speaker GAMM (created in "analysis/main-analyses.Rmd")
load("analysis/gamms/gam_ns.rda")


# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Log-likelihood of data sets under native model --------------------------
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# TO DO
# - FUnction with log-odds rather than log-likelihood
# - Q to Flo: sum LLs or mean??

# Function
mypredict <- function(d_new = NULL, N = 100, d_nat = d_ns, mygam = gam_ns) {
  # d_new is the new data from which the response value is to be predicted
  
  # Data frame with the actual subjects from which values are predicted
  pred <- d_new %>% group_by(Subject, Condition, ClozeScore) %>% summarise()
  # The matrix that will contain the predicted values in each loop
  store_values <- matrix(nrow = length(unique(d_new$Subject)), ncol = N)
  # Need to keep track of the real subjects before assigning them new SubjIDs
  d_new$TrueSubject <- d_new$Subject
  
  for (n in 1:N) {
    # Override subject info by randomly assigning native subject IDs with repl.
    levels(d_new$Subject) <- sample(unique(d_nat$Subject), replace = TRUE,
                                    size = length(unique(d_new$Subject)))
    # predict participant data under native model (includes subject smooths)
    d_new$predicted <- predict.gam(mygam, newdata = d_new, type = "response")
    # log the probability of each trial and then average these log-probabilities
    # by-participant, giving you log-likelihoods (LL) of each participant being
    # generated by the native model:
    d_new$ll <- with(d_new, log(ifelse(Used == 1, predicted, 1 - predicted)))
    store_values[, n] <- d_new %>% group_by(TrueSubject) %>%
      summarise(LL = mean(ll)) %>% pull(LL)
  }
  # The N random assignments lead to a distribution of LL per subject
  myConfint <- t(apply(store_values, 1, quantile, probs = c(.05, .5, .95)))
  pred <- data.frame(pred, myConfint)
  names(pred)[4:6] <- paste("quant", c("05", "50", "95"), sep = "")
  pred
}

# run and time
ptm <- proc.time()
l2_under_natmodel <- mypredict(d_new = d_l2)
ns_under_natmodel <- mypredict(d_new = d_ns)
proc.time() - ptm

## combine native and non-native predictions
l2_under_natmodel$Group <- "L2"
ns_under_natmodel$Group <- "NS"

# save(l2_under_natmodel, file = "analysis/gamms/l2_under_natmodel.rda")
# save(ns_under_natmodel, file = "analysis/gamms/ns_under_natmodel.rda")

load(file = "analysis/gamms/l2_under_natmodel.rda")
load(file = "analysis/gamms/ns_under_natmodel.rda")


# predict the actual native speaker data under the native model without
# reassigning the subject IDs! (This should yield the highest log-likelihood)
d_ns$predicted <- predict.gam(gam_ns, newdata = d_ns, type = "response")
d_ns$ll <- with(d_ns, log(ifelse(Used == 1, predicted, 1 - predicted)))
# Store as dataframe with same shape as the two above
ns_actual_under_natmodel <- d_ns %>%
  group_by(Subject, Condition, ClozeScore) %>% 
  summarise(quant50 = mean(ll)) %>% 
  mutate(quant05 = NA, quant95 = NA, Group = "ActualNS") %>%
  select(Subject:ClozeScore, quant05, quant50, quant95:Group)



# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Visualize ---------------------------------------------------------------
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

## L2ers

# L2 data sorted by Proficiency (Cloze score)
l2_under_natmodel %>%
  ggplot(aes(x = ClozeScore, y = quant50, colour = Condition)) +
  geom_point() +
  geom_smooth(method = "lm")

# With confidence intervals
l2_under_natmodel %>%
  ggplot(aes(x = ClozeScore, y = quant50, colour = Condition,
             ymin = quant05, ymax = quant95)) +
  geom_point(position = position_dodge(width=0.5)) +
  geom_errorbar(width=0, position=position_dodge(width=0.5)) + #), position=position_dodge(.9))
  geom_smooth(method = "lm")

# Native speakers with randomly assigned subject numbers
ns_under_natmodel %>%
  ggplot(aes(x = "NULL", y = quant50, colour = Condition)) +
  geom_boxplot()


# Combined predictions
combined_preds <- rbind(l2_under_natmodel, ns_under_natmodel,
                        data.frame(ns_actual_under_natmodel))

combined_preds %>%
  ggplot(aes(x = Condition, y = quant50, colour = Group)) +
  geom_boxplot()

# The simplifying assumption behind this is that we're assessing non-native
# subject's 'nativeness' (likelihood) while assuming that the sample of random
# differences among natives is sufficiently representative of the population.
# I think that's ok (but the CIs will also give you an idea how much of a
# non-native  subject's 'nativeness' depends on the random by-subject variances).

# if we so fancy, one can analyze the LLs with a gamm or glmm, but perhaps
# the visualization does the job?

